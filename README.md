# Тестовое задание: Demo-Detection

Данный репозиторий представляет собой решение к тестовому заданию на детекцию
ТС легкового и грузового типа на панорамном ЧБ видео. Он включает в себя код
для обучения и экспорта модели, анализа её качества
(src/py/solution_pipeline.ipynb) и трекинга ТС с её помощью (src/py/track.py).

## Обучение модели и эксперименты

Обучение финальной модели (дообучение YOLO26m) было проведено на M4 Pro MacBook
Pro (24G Unified Memory) за 13 эпох. Также были проведены эксперименты с
дообучением YOLO26l на той же машине за 18 эпох и YOLO26m на Debian (WSL2) c
RTX5060 (8G VRAM) за 20 эпох. Несмотря на более высокие метрики моделей,
дообучившихся за большее количество эпох, наиболее адекватный результат
показала YOLO26m за 13 эпох. Именно она в отличие от остальных моделей смогла
определить грузовое ТС на тестовом видео.

Весь код и сопроводительные комментарии и наблюдения находятся в
src/py/solution_pipeline.ipynb. **Имейте в виду**, что в экспортной версии
тетради (для открытого репозитория) показан пример с дообучением на RTX5060.

Чтобы запустить код на Python, необходимо в рабочей директории с
проектом выполнить:
```shell
uv sync
source .venv/bin/activate
```

**Примечание:** Если в системе нет UV, установите его:
```shell
wget -qO- https://astral.sh/uv/install.sh | sh
```

Далее уже переходите в папку с кодом на Python и оттуда либо
`jupyter notebook .`, либо `python track.py`. **Обратите внимание!** Перед
запуском track.py следует отредактировать путь к модели и видео для детекции.

Чтобы запустить процесс обучения в Jupyter-блокноте, необходимо сначала
в корневой папке проекта создать директорию "dataset", куда будут распакованы
исходные данные (test_task_dl_26_12_2025.zip).

## Сборка приложения на C++

Клонируем настоящий репозиторий:
```shell
git clone https://github.com/mkashirin/demo-detection
```

Для сборки приложения на C++ потребуются:
1. [NVIDIA CUDA Toolkit 12.x](
    https://developer.nvidia.com/cuda-12-9-1-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Debian&target_version=12&target_type=deb_local
),
2. OpenCV 4.10.0 (libopencv-dev),
3. [Argparse 3.2](
    https://github.com/p-ranav/argparse/archive/refs/tags/v3.2.zip
),
4. [ONNX Runtime 1.23.2](
    https://github.com/microsoft/onnxruntime/releases/download/v1.23.2/onnxruntime-linux-x64-gpu-1.23.2.tgz
)

Первый пункт имеет чёткую инструкцию по установке, доступную по гиперссылке
(обязательно ставьте проприетарную версию драйверов). OpenCV устанавливается из
репозиториев Debian (для поддержки всех кодеков и фишек):
```shell
sudo apt-get install libopencv-dev
```

Перед установкой Argparse и ONNX Rutime нужно будет в рабочей директории
с проектом создать папку "vendor":
```shell
mkdir vendor && cd vendor
```

Argparse собирается и тестируется следующим образом:
```shell
wget https://github.com/p-ranav/argparse/archive/refs/tags/v3.2.zip
unzip v3.2.zip && cd argparse-3.2
mkdir build && cd build
cmake -DARGPARSE_BUILD_SAMPLES=on -DARGPARSE_BUILD_TESTS=on ..
make -j8
./test/tests
```

ONNX Runtime уже собран и готов к использованию. Поэтому его достаточно просто
распаковать (в директорю "vendor"):
```shell
wget https://github.com/microsoft/onnxruntime/releases/download/v1.23.2/onnxruntime-linux-x64-gpu-1.23.2.tgz
tar -xvf onnxruntime-linux-x64-gpu-1.23.2.tgz 
```

После этого из рабочей директории с проектом выполняем следующие строки:
```shell
mkdir build && cd build
cmake ..
make -j8
```

**Примечание:** ОЧЕНЬ ВАЖНО установить правильную версию CUDA Toolkit (12.x),
совместимую с ONNX Runtime (1.23.2).

## Запуск приложения на С++

Скачайте и разархивируйте директорию с экспортными файлами в рабочую
директорию с бинарным файлом (если все шаги по сборке были выполнены верно, это
будет папка "build"):
```shell
wget https://storage.yandexcloud.net/lab-storage/export.zip
unzip export.zip
```

Запустите приложение следующим образом:
```shell
./demo_detection \
    --model_path export/demo_detection_v4e13.onnx \
    --input_data export/video_test/10.0s_video_test.mp4 \
    --output_data 10.0s_video_test_cpp.mp4
```

В одной директории с бинарным файлом должен будет появиться файл под названием
"10.0s_video_test_cpp.mp4", который и является резульатом отработки модели
детекции.

## Рекомендации и дальнейшие улучшения

Самое элементарное, что можно было бы сделать, это прогнать процесс дообучение
на большее количество эпох. Возможно, это бы всё-таки позволило моделям
покрупнее лучше различать ТС.

Далее, если речь идёт о большом количестве эпох, можно было бы всё же добавить
изображения с трейлерами (которые были вырезаны, чтобы облегчить задачу
детекции для модели). Точно так же появляется смысл в увеличении размера
обучающей выборки путём аугментации, в использовании метаданных из файлов
CVAT-аннотаций.

Что касается приложения на C++, можно было бы поискать способы статически
привязать максимально возможное количество библиотек для портативности.
Функционал для динамической смены фреймворка для выполнения модели тоже не
оказался бы лишним, так как на более свежих видеокартах есть доступ к TensorRT,
который быстрее ONNX и Libtorch.

## Почему ONNX, а не Libtorch

Решение об экспорте модели в ONNX было принято на основе условий задания о
совместимости с видеокартами GT1000+ серии. Конечно, реализация с Libtorch,
возможно, оказалась бы более простой и производительной, однако в таком
случае приложение теряет в портативности и совместимости с более древними
системами. Также ONNX имеет fallback на CPU в том случае, если нет правильных
драйверов для GPU. А это значит, что приложение запустится с большей
вероятностью даже на свосем legacy системе.
